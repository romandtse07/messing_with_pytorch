{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "\n",
    "torch.Tensor classes are the tensors, obviously.  defining variables in terms of tensors that were operated on makes that variable carry a gradient function if the independent tensor has the requires_grad argument set to True.  Calling the .backward(torch.tensor(1's the same shape as the object calling this)) backpropogates the gradients onto the component tensors' .grad parameter.  These accumulate the effects of calling .backward(), i assume for the flexibility of having several outputs depend on a single tensor.  Can also stop a certain block from accumulating anything from .backward() calls that automatically happen in neural net training.  and there's spit on the table.  thank you santa monica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[                                       0.0000,\n",
       "                                                2.0000],\n",
       "        [-218072553553409255625889024007745830912.0000,\n",
       "                                                0.0000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8108, 0.6580, 0.5105],\n",
       "        [1.0203, 1.4723, 1.5471]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2,3) + torch.rand(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2,2)\n",
    "y = torch.tensor([[0,0],\n",
    "                  [1,2]], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4500, 0.2885],\n",
       "        [4.2379, 8.0516]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x, y, out=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4500,  0.2885],\n",
       "        [ 7.2379, 14.0516]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#another way to add inplace\n",
    "x.add_(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in place operators will all end in an underscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8513, 0.7959],\n",
       "        [0.8814, 0.9536],\n",
       "        [0.9854, 0.0006]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#slicing\n",
    "torch.rand(3,3)[:,1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6029, -1.1257,  0.8131, -0.3767],\n",
       "        [ 0.8756,  0.3649,  0.8592, -0.0926],\n",
       "        [ 0.6054, -0.5257,  1.3566, -0.7312],\n",
       "        [-0.1729, -1.1118, -1.6253, -0.3349]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6029, -1.1257,  0.8131, -0.3767,  0.8756,  0.3649,  0.8592, -0.0926,\n",
       "         0.6054, -0.5257,  1.3566, -0.7312, -0.1729, -1.1118, -1.6253, -0.3349])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6029, -1.1257,  0.8131, -0.3767,  0.8756,  0.3649,  0.8592, -0.0926],\n",
       "        [ 0.6054, -0.5257,  1.3566, -0.7312, -0.1729, -1.1118, -1.6253, -0.3349]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unwraps left to right, top to bottom.  view is the new reshape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6028594970703125"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's how you take out one element from a tensor i guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to numpy\n",
    "a = x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.6028595 , -1.1256753 ,  0.81312853, -0.3767377 ],\n",
       "       [ 0.87557375,  0.3649384 ,  0.8592395 , -0.0925917 ],\n",
       "       [ 0.6053944 , -0.5256794 ,  1.3566024 , -0.73117405],\n",
       "       [-0.17291704, -1.1118176 , -1.6253008 , -0.3349216 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from numpy\n",
    "b = torch.from_numpy(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6029, -1.1257,  0.8131, -0.3767],\n",
       "        [ 0.8756,  0.3649,  0.8592, -0.0926],\n",
       "        [ 0.6054, -0.5257,  1.3566, -0.7312],\n",
       "        [-0.1729, -1.1118, -1.6253, -0.3349]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6029, -1.1257,  0.8131, -0.3767],\n",
       "        [ 0.8756,  0.3649,  0.8592, -0.0926],\n",
       "        [ 0.6054, -0.5257,  1.3566, -0.7312],\n",
       "        [-0.1729, -1.1118, -1.6253, -0.3349]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6029, -1.1257,  0.8131, -0.3767,  0.8756,  0.3649,  0.8592, -0.0926,\n",
       "         0.6054, -0.5257,  1.3566, -0.7312, -0.1729, -1.1118, -1.6253, -0.3349])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6029, -1.1257,  0.8131, -0.3767],\n",
       "        [ 0.8756,  0.3649,  0.8592, -0.0926],\n",
       "        [ 0.6054, -0.5257,  1.3566, -0.7312],\n",
       "        [-0.1729, -1.1118, -1.6253, -0.3349]], requires_grad=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this happens in place\n",
    "x.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#appears to be elementwise multiplication\n",
    "y = x*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3634, 1.2671, 0.6612, 0.1419],\n",
       "        [0.7666, 0.1332, 0.7383, 0.0086],\n",
       "        [0.3665, 0.2763, 1.8404, 0.5346],\n",
       "        [0.0299, 1.2361, 2.6416, 0.1122]], grad_fn=<ThMulBackward>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ThMulBackward at 0x115d299e8>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when you arrive at a scalar output, you can take the gradient with the .backward() method\n",
    "rmse_kinda = torch.sum((y - y.mean())**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates current gradients for any element in a tensor that required gradients\n",
    "#same as .backward(torch.tensor(1))\n",
    "#autograd function it's called\n",
    "rmse_kinda.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -0.7992,  -2.5768,  -0.1096,   0.8333],\n",
       "        [  0.2513,  -0.8199,   0.1492,   0.2542],\n",
       "        [ -0.7952,   0.8801,   6.2159,   0.4687],\n",
       "        [  0.4599,  -2.4071, -12.6561,   0.7806]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([4,5,6], dtype=torch.float)\n",
    "\n",
    "y = 3*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 5., 6.], requires_grad=True)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this won't work. have to reinstantiate it\n",
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.transpose(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 5., 6.], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TransposeBackward0 at 0x115d50fd0>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(torch.tensor([1,1,1], dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "running this repeatedly seems to keep bumping up the gradient.  has to move forward at some point i guess.  time for sleep now.  also note scalars required backward with rank 0 tensor as argument (the default).  when y was rank 1, passing in a tensor with all ones and the same dimensions produced gradients for x components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Neural Network using PyTorch Modules\n",
    "\n",
    "playing off of and taking notes on the pytorch tutorial.  Here is a rip from the tutorial summarizing the objects seen here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap:\n",
    "\n",
    "torch.Tensor - A multi-dimensional array with support for autograd operations like backward(). Also holds the gradient w.r.t. the tensor.\n",
    "\n",
    "nn.Module - Neural network module. Convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc.\n",
    "\n",
    "nn.Parameter - A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to a Module.\n",
    "\n",
    "autograd.Function - Implements forward and backward definitions of an autograd operation. Every Tensor operation, creates at least a single Function node, that connects to functions that created a Tensor and encodes its history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as fxn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extending basic neural network class from torch.nn\n",
    "# knows what to do with backward, but need to define forward method and layers during init\n",
    "class ANN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_len, hidden_nodes):\n",
    "        # call nn.Module's init.  in hindsight i guess this should have to happen or else overwrite i guess\n",
    "        super(ANN, self).__init__()\n",
    "        self.fc = nn.Linear(input_len, hidden_nodes)\n",
    "        self.out = nn.Linear(hidden_nodes, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = fxn.relu(self.fc(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic = ANN(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN(\n",
      "  (fc): Linear(in_features=1, out_features=10, bias=True)\n",
      "  (out): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(generic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0904],\n",
       "         [ 0.0597],\n",
       "         [ 0.0084],\n",
       "         [-0.2758],\n",
       "         [ 0.2681],\n",
       "         [ 0.8497],\n",
       "         [-0.9456],\n",
       "         [ 0.9478],\n",
       "         [-0.3208],\n",
       "         [-0.7305]], requires_grad=True), Parameter containing:\n",
       " tensor([ 0.6177,  0.1303,  0.2869,  0.2174,  0.4244, -0.0081, -0.9745,  0.1199,\n",
       "          0.0399,  0.0681], requires_grad=True), Parameter containing:\n",
       " tensor([[-0.1220,  0.1886,  0.0749, -0.0425, -0.0337, -0.1091, -0.2052, -0.1783,\n",
       "          -0.1361,  0.1148]], requires_grad=True), Parameter containing:\n",
       " tensor([0.2503], requires_grad=True)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(generic.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the parameter attribute for a module class.  note that this is some weird new object that knows it's a parameter, not just a tensor.  things i took for granted: \n",
    "- all layers require gradients by default for obvious reasons.\n",
    "- parameters include going from input to layer, layer biases, layers to output, output bias\n",
    "- weights initialized for me, ranging from zero to one.  Hence why normalization is good.  Not to mention everything is thrown into this network GLM style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
